---
description: Guidelines for performance optimization
globs: ["*.py", "*.sql", "*.json", "*.yaml", "*.yml"]
alwaysApply: true
---
# Performance Guidelines

Rules for consistent performance optimization across the codebase.

<rule>
name: performance-optimization
description: Standards for optimizing performance and resource utilization
filters:
  # Match Python files
  - type: file_extension
    pattern: "\\.py$"
  # Match SQL files
  - type: file_extension
    pattern: "\\.sql$"
  # Match config files
  - type: file_extension
    pattern: "\\.(json|ya?ml)$"
  # Match performance related content
  - type: content
    pattern: "(performance|optimize|cache|index|batch|pool|concurrent|resource|memory|cpu|latency|throughput|bottleneck|profile|benchmark|tune)"

actions:
  - type: reject
    conditions:
      # Reject inefficient queries
      - pattern: "SELECT\\s+\\*\\s+FROM"
        message: "Specify required columns instead of using SELECT *"
      # Reject nested loops
      - pattern: "for.*\\s+for.*:"
        message: "Avoid nested loops. Consider vectorized operations or better algorithms."
      # Reject unbounded queries
      - pattern: "SELECT.*(?<!LIMIT\\s+\\d+)$"
        message: "Include LIMIT clause in SELECT queries"
      # Reject inefficient string concatenation
      - pattern: "\\s*\\+\\s*\\.join\\("
        message: "Use list comprehension or generator expressions instead of string concatenation"
      # Reject large in-memory operations
      - pattern: "\\.to_list\\(\\)|list\\(.*\\)"
        message: "Avoid converting large iterables to lists. Use generators or iterators."
      # Reject inefficient pandas operations
      - pattern: "df\\.apply\\(.*\\)"
        message: "Use vectorized operations instead of df.apply"
      # Reject unbounded memory growth
      - pattern: "append|extend(?!.*max_size)"
        message: "Implement size limits for growing collections"
      # Reject unoptimized file operations
      - pattern: "open\\(.*\\)(?!.*buffering)"
        message: "Specify appropriate buffer size for file operations"

  - type: suggest
    message: |
      When optimizing performance:

      1. Query Optimization:
         - Use appropriate indexes
         - Optimize JOIN operations
         - Minimize data scanning
         - Use efficient filtering
         - Implement query caching
         - Monitor query plans
         - Use query hints when needed

      2. Resource Management:
         - Batch operations when possible
         - Use appropriate chunk sizes
         - Monitor memory usage
         - Handle resource constraints
         - Implement connection pooling
         - Track resource utilization
         - Set resource limits

      3. Caching:
         - Implement result caching
         - Use appropriate cache TTL
         - Handle cache invalidation
         - Monitor cache hit ratio
         - Implement cache warming
         - Log cache performance
         - Use distributed caching

      4. Concurrency:
         - Handle parallel operations
         - Manage connection pools
         - Implement rate limiting
         - Handle deadlocks
         - Monitor thread usage
         - Track concurrent operations
         - Use async operations

      5. Monitoring:
         - Track operation timing
         - Monitor system resources
         - Log performance metrics
         - Identify bottlenecks
         - Track optimization impact
         - Monitor system health
         - Set up alerts

      6. Data Structures:
         - Choose appropriate containers
         - Implement efficient algorithms
         - Use memory-efficient types
         - Optimize data layout
         - Handle large datasets
         - Use specialized structures
         - Profile memory usage

      7. Code Optimization:
         - Use profiling tools
         - Implement benchmarking
         - Optimize hot paths
         - Remove redundant operations
         - Use appropriate algorithms
         - Cache expensive computations
         - Document optimizations

examples:
  - input: |
      # Bad: Performance anti-patterns
      def process_data():
          results = []  # Unbounded list growth
          
          # Inefficient data loading
          with open('large_file.txt') as f:  # No buffer size
              data = f.readlines()  # Loads entire file into memory
          
          # Multiple inefficient operations
          for id in ids:  # N+1 queries
              result = db.query(f"SELECT * FROM table WHERE id = {id}")  # Inefficient query
              results.append(result)  # Unbounded growth
          
          # Inefficient pandas operations
          df['new_col'] = df.apply(lambda x: expensive_computation(x), axis=1)  # Use vectorization
          
          # Inefficient string operations
          output = ""
          for item in results:
              output += str(item) + "\n"  # String concatenation
          
          return output

      # Good: Optimized implementation with comprehensive monitoring
      def process_data_optimized(ids, batch_size=1000):
          # Set up performance monitoring
          with PerformanceMonitor() as monitor:
              # Initialize metrics
              metrics = {
                  'start_time': time.time(),
                  'batches': 0,
                  'total_records': 0,
                  'cache_hits': 0
              }
              
              try:
                  # Use connection pooling
                  with get_connection_from_pool() as conn:
                      # Use generators for memory efficiency
                      def process_batches():
                          for batch_ids in chunk_list(ids, batch_size):
                              with monitor.measure('batch_processing'):
                                  # Check cache first
                                  cached_results = cache.get_many(batch_ids)
                                  metrics['cache_hits'] += len(cached_results)
                                  
                                  # Query missing items efficiently
                                  missing_ids = [id for id in batch_ids if id not in cached_results]
                                  if missing_ids:
                                      # Efficient querying with proper indexes
                                      query = """
                                          SELECT id, name, value 
                                          FROM table 
                                          WHERE id = ANY(%s)
                                          AND status = 'active'
                                          LIMIT %s
                                      """
                                      with monitor.measure('database_query'):
                                          batch_results = conn.execute(
                                              query,
                                              (missing_ids, batch_size)
                                          )
                                      
                                      # Cache new results
                                      cache.set_many({r['id']: r for r in batch_results})
                                  
                                  # Combine cached and new results
                                  yield from itertools.chain(
                                      cached_results.values(),
                                      batch_results if missing_ids else []
                                  )
                              
                              # Track metrics
                              metrics['batches'] += 1
                              metrics['total_records'] += len(batch_ids)
                      
                      # Process data with vectorized operations
                      df = pd.DataFrame(process_batches())
                      
                      # Optimize expensive computations
                      with monitor.measure('computation'):
                          df['computed_value'] = vectorized_computation(df)
                      
                      # Efficient string building
                      return '\n'.join(
                          df.to_string(index=False).splitlines()
                      )
              
              finally:
                  # Log comprehensive metrics
                  metrics.update({
                      'total_time': time.time() - metrics['start_time'],
                      'cache_hit_ratio': metrics['cache_hits'] / metrics['total_records'],
                      'avg_batch_time': monitor.get_average('batch_processing'),
                      'avg_query_time': monitor.get_average('database_query'),
                      'memory_usage': monitor.get_memory_usage(),
                      'cpu_usage': monitor.get_cpu_usage()
                  })
                  
                  # Log metrics with appropriate level
                  if metrics['avg_query_time'] > QUERY_TIME_THRESHOLD:
                      logging.warning("Slow query performance", extra=metrics)
                  else:
                      logging.info("Performance metrics", extra=metrics)

metadata:
  priority: high
  version: 1.1
  tags:
    - performance
    - optimization
    - monitoring
    - profiling
    - caching
  changelog:
    - 1.0: Initial version with comprehensive performance optimization guidelines
    - 1.1: Added profiling, data structures, and code optimization guidelines 