---
description: 
globs: 
alwaysApply: false
---
# Logging and Monitoring Guidelines

## Overview
Standards for implementing comprehensive logging and monitoring across the project.

## Configuration
globs: ["*.py", "*.sql", "*.sh"]
priority: high

## Rules

### Logging Standards
1. Use appropriate log levels
   - ERROR: For errors that need immediate attention
   - WARNING: For potentially harmful situations
   - INFO: For general operational information
   - DEBUG: For detailed debugging information

2. Include essential context
   - Timestamp
   - Log level
   - Component/Module name
   - Process/Thread ID
   - User/Request ID when applicable

3. Structure log messages
   - Be consistent with message format
   - Include relevant data
   - Avoid sensitive information
   - Use machine-parseable format

### Monitoring Requirements
1. System Metrics
   - CPU usage
   - Memory utilization
   - Disk space
   - Network I/O

2. Application Metrics
   - Request rates
   - Error rates
   - Response times
   - Active users/connections

3. Data Pipeline Metrics
   - Records processed
   - Processing time
   - Error counts
   - Data quality metrics

## Examples

### Python Logging
```python
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/tcdatalogger/app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def process_data(data: dict) -> None:
    """Process data with comprehensive logging."""
    try:
        logger.info("Starting data processing", extra={
            "records": len(data),
            "process_id": os.getpid()
        })
        
        result = transform_data(data)
        
        logger.info("Data processing complete", extra={
            "processed_records": len(result),
            "processing_time": processing_time
        })
    except Exception as e:
        logger.error("Data processing failed", exc_info=True, extra={
            "error_type": type(e).__name__,
            "error_details": str(e)
        })
```

### Monitoring Metrics
```python
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ProcessingMetrics:
    timestamp: datetime
    records_processed: int
    processing_time: float
    error_count: int
    data_quality_score: float

def log_metrics(metrics: ProcessingMetrics) -> None:
    """Log processing metrics to monitoring system."""
    logger.info("Processing metrics", extra={
        "metric_type": "processing_stats",
        "timestamp": metrics.timestamp.isoformat(),
        "records_processed": metrics.records_processed,
        "processing_time_ms": metrics.processing_time * 1000,
        "error_count": metrics.error_count,
        "quality_score": metrics.data_quality_score
    })
```

## Version History
- 1.0: Initial version
- 1.1: Added specific metrics for Torn City data processing 